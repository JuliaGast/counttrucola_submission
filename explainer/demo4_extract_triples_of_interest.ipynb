{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa86454",
   "metadata": {},
   "source": [
    "# Extract certain quadruples that should be explained and store them as input for the explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d53fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import util_scripts.stats_utils as su\n",
    "import util_scripts.dataset_utils as du\n",
    "sys.path.append('..')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project root directory\n",
    "sys.path.append(os.path.abspath(\"../rule_based\"))  # or the absolute path to your project\n",
    "\n",
    "from rule_based.rule_dataset import RuleDataset\n",
    "from rule_based.eval import evaluate\n",
    "import rule_utils\n",
    "from tgb.linkproppred.evaluate import Evaluator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06aa7f",
   "metadata": {},
   "source": [
    "## For given rankings, and ids, find all (test/val) quadruples that have reciprocal rank (rr) in between given thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b82aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw file found, skipping download\n",
      "Dataset directory is  c:\\Users\\jgasting\\PythonScripts\\Rules\\GraphTRuCoLa\\tgb/datasets\\tkgl_icews14\n",
      "loading processed file\n",
      "num_rels:  230\n",
      ">>> loading and indexing of dataset 2.896 seconds\n",
      ">>> average number of time steps for a triple: 1.804\n",
      ">>> checked order of time steps, everything is fine\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tkgl-icews14'\n",
    "rankings_filename ='tkgl-icews14-rankings_test_conf_0_corr_conf_0_noisyor_crules_frules_zrules_pvalue_30_num_top_rules_10_multi.txt'  # replace with name of your rankings file -\n",
    "eval_mode ='test'\n",
    "rankings_arules_name = os.path.join('..', 'files', 'rankings', dataset_name, rankings_filename)\n",
    "rule_dataset =  RuleDataset(name=dataset_name)\n",
    "\n",
    "upper_limit = 1 # rr < upper_limit\n",
    "lower_limit = 0.2 # rr > lower_limit\n",
    "\n",
    "exp_name = 'compare'\n",
    "outpath = os.path.join('..', 'files', 'explanations', exp_name, 'input')\n",
    "\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "explanations_path = os.path.join(outpath, \"quadruples.txt\")  # this can be used as input for the explainer\n",
    "\n",
    "src_of_interest = 'all'\n",
    "dst_of_interst = 'all'\n",
    "t_of_interest = 'all'\n",
    "rel_of_interest = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b4b871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading negative test samples\n"
     ]
    }
   ],
   "source": [
    "dataset = rule_dataset.dataset\n",
    "num_nodes = rule_dataset.dataset.num_nodes\n",
    "split_mode = eval_mode\n",
    "evaluator = Evaluator(name=dataset.name, k_value=[1,10,100])\n",
    "neg_sampler = dataset.negative_sampler  \n",
    "\n",
    "if eval_mode == \"val\":\n",
    "    testdata = rule_dataset.val_data\n",
    "    print(\"loading negative val samples\")\n",
    "    dataset.load_val_ns() # load negative samples, i.e. the nodes that are not used for time aware filter mrr\n",
    "elif eval_mode == \"test\":\n",
    "    testdata = rule_dataset.test_data\n",
    "    print(\"loading negative test samples\")\n",
    "    dataset.load_test_ns() # load negative samples, i.e. the nodes that are not used for time aware filter mrr\n",
    "\n",
    "rankings_arules = rule_utils.read_rankings_order(rankings_arules_name, num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> starting evaluation for every triple, in the  test set\n",
      "in total we had 63 cases mrr was in threshold\n",
      "explanations written to ..\\files\\explanations\\compare\\input\\quadruples.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('>>> starting evaluation for every triple, in the ', eval_mode, 'set')\n",
    "total_iterations = len(testdata)\n",
    "progressbar_percentage = 0.01\n",
    "\n",
    "\n",
    "increment = int(total_iterations*progressbar_percentage) if int(total_iterations*progressbar_percentage) >=1 else 1\n",
    "remaining = total_iterations\n",
    "\n",
    "in_threshold_counter = 0\n",
    "\n",
    "with open(explanations_path, 'w') as file_explanations:\n",
    "    # with tqdm(total=total_iterations) as pbar:\n",
    "    counter = 0\n",
    "    file_explanations.write(\"subject rel object timestep\\n\")\n",
    "    for i, (src, dst, t, rel) in enumerate(zip(testdata[:,0], testdata[:,2], testdata[:,3], testdata[:,1])):\n",
    "        # only do this for src, dst, t, rel of interest\n",
    "        if not src_of_interest == 'all':\n",
    "            if not src in src_of_interest:\n",
    "                continue\n",
    "        if not dst_of_interst == 'all':\n",
    "            if not dst in dst_of_interst:\n",
    "                continue\n",
    "        if not rel_of_interest == 'all':\n",
    "            if not rel in rel_of_interest:\n",
    "                continue\n",
    "        if not t_of_interest == 'all':\n",
    "            if not t in t_of_interest:\n",
    "                continue\n",
    "\n",
    "        # Update progress bar\n",
    "\n",
    "            \n",
    "        original_t = rule_dataset.timestamp_id2orig[t]\n",
    "\n",
    "        # Query negative batch list - all negative samples for the given positive edge that are not temporal conflicts (time aware mrr)\n",
    "        neg_batch_list = neg_sampler.query_batch(np.array([src]), np.array([dst]), np.array([original_t]), edge_type=np.array([rel]), split_mode=split_mode)\n",
    "\n",
    "        # Make predictions for given src, rel, t\n",
    "        # Compute a score for each node in neg_batch_list and for actual correct node dst\n",
    "        scores_array_arules =rule_utils.create_scores_array(rankings_arules[(src, rel, t)], num_nodes)\n",
    "\n",
    "        #### a_rules\n",
    "        predictions_neg_arules = scores_array_arules[neg_batch_list[0]]\n",
    "        predictions_pos_arules = np.array(scores_array_arules[dst])\n",
    "        # Evaluate the predictions\n",
    "        input_dict = {\n",
    "            \"y_pred_pos\": predictions_pos_arules,\n",
    "            \"y_pred_neg\": predictions_neg_arules,\n",
    "            \"eval_metric\": ['mrr'], \n",
    "        }\n",
    "        predictions_arules = evaluator.eval(input_dict)\n",
    "        mrr_arule = float(predictions_arules['mrr'])\n",
    "        hits10_arule = float(predictions_arules['hits@10'])\n",
    "        hits1_arule = float(predictions_arules['hits@1'])\n",
    "        hits100_arule = float(predictions_arules['hits@100'])\n",
    "        #### check if all conditions are met to have b-predictiion [significantly] better than a-prediction\n",
    "        if mrr_arule < upper_limit: #1) the correct candidate in b has a higher rank than in a\n",
    "            if mrr_arule > lower_limit:\n",
    "                file_explanations.write(str(src) + \" \" + str(rel) + \" \" + str(dst) + \" \" + str(t) + '\\n')\n",
    "                in_threshold_counter +=1\n",
    "\n",
    "print(\"in total we had\", (in_threshold_counter), \"cases mrr was in threshold\")\n",
    "print(\"explanations written to\", explanations_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
